# ë‹¤ì¤‘ ì‚¬ìš©ì ë™ì‹œ ì ‘ì† ê´€ë¦¬ ì „ëµ ë¶„ì„

## ğŸ“Š í˜„ì¬ êµ¬í˜„ í˜„í™© (Current Implementation)

### âœ… ì´ë¯¸ êµ¬í˜„ëœ ê¸°ëŠ¥

#### 1. **ì„¸ì…˜ ê´€ë¦¬ (Session Management)**
**êµ¬í˜„ íŒŒì¼**: `backend/app/core/session_store.py`

**í˜„ì¬ ê¸°ëŠ¥**:
- âœ… **Thread-safe ì„¸ì…˜ ì €ì¥ì†Œ**: `asyncio.Lock` per session
- âœ… **ì„¸ì…˜ë³„ ê²©ë¦¬**: ê° ì„¸ì…˜ì€ ë…ë¦½ì ì¸ ë½ì„ ê°€ì§
- âœ… **ì„¸ì…˜ë³„ í”„ë ˆì„ì›Œí¬ ì„ íƒ**: standard/deepagents ë…ë¦½ ê´€ë¦¬
- âœ… **ì„¸ì…˜ë³„ ì›Œí¬ìŠ¤í˜ì´ìŠ¤**: ê° ì‚¬ìš©ìëŠ” ë³„ë„ì˜ ì‘ì—… ê³µê°„
- âœ… **ì„¸ì…˜ ì •ë³´ ì¡°íšŒ/ì‚­ì œ**: list_sessions(), delete_session()

**êµ¬í˜„ ë°©ì‹**:
```python
class SessionStore:
    def __init__(self):
        self._frameworks: Dict[str, FrameworkType] = {}
        self._workspaces: Dict[str, str] = {}
        self._locks: Dict[str, asyncio.Lock] = defaultdict(asyncio.Lock)
        self._global_lock = asyncio.Lock()  # For lock management
```

**ì¥ì **:
- ì„¸ì…˜ë³„ ë½ìœ¼ë¡œ **fine-grained locking** â†’ ë‹¤ë¥¸ ì„¸ì…˜ì€ ë™ì‹œ ì²˜ë¦¬ ê°€ëŠ¥
- Race condition ë°©ì§€
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (sessionë‹¹ í•˜ë‚˜ì˜ Lockë§Œ ìƒì„±)

#### 2. **ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ê´€ë¦¬ (Workspace Management)**
**êµ¬í˜„ íŒŒì¼**: `backend/app/services/workflow_service.py`

**í˜„ì¬ ê¸°ëŠ¥**:
- âœ… **ì„¸ì…˜ë³„ ë…ë¦½ ì›Œí¬ìŠ¤í˜ì´ìŠ¤**: `/home/user/workspace/project_name_<session>`
- âœ… **í”„ë¡œì íŠ¸ëª… ìë™ ìƒì„±**: LLMì´ ì‚¬ìš©ì ìš”ì²­ ê¸°ë°˜ìœ¼ë¡œ ì œì•ˆ
- âœ… **ì¤‘ë³µ ë°©ì§€**: ê°™ì€ ì´ë¦„ í”„ë¡œì íŠ¸ ì¡´ì¬ ì‹œ `_1`, `_2` ì ‘ë¯¸ì‚¬ ì¶”ê°€
- âœ… **ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì¬ì‚¬ìš©**: ê°™ì€ ì„¸ì…˜ì€ ê¸°ì¡´ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì¬ì‚¬ìš©
- âœ… **Path Traversal ë°©ì§€**: `sanitize_path()` ë³´ì•ˆ ê²€ì¦

**êµ¬í˜„ ë°©ì‹**:
```python
async def get_or_create_workspace(
    session_id: str,
    user_message: str,
    base_workspace: Optional[str] = None
) -> str:
    # ê¸°ì¡´ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì¬ì‚¬ìš©
    existing_workspace = await self.session_store.get_workspace(session_id, default=None)
    if existing_workspace:
        return existing_workspace

    # ìƒˆ í”„ë¡œì íŠ¸ ìƒì„±
    project_name = await self.suggest_project_name(user_message)
    workspace = os.path.join(workspace_root, project_name)

    # ì¤‘ë³µ ë°©ì§€
    while os.path.exists(workspace):
        workspace = os.path.join(workspace_root, f"{project_name}_{counter}")
        counter += 1
```

**ì¥ì **:
- ì‚¬ìš©ìê°„ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì™„ì „ ê²©ë¦¬
- íŒŒì¼ ì¶©ëŒ ë°©ì§€
- ë³´ì•ˆ: Path traversal ê³µê²© ì°¨ë‹¨

#### 3. **ë°ì´í„°ë² ì´ìŠ¤ ë™ì‹œì„± (Database Concurrency)**
**êµ¬í˜„ íŒŒì¼**: `backend/app/db/database.py`

**í˜„ì¬ ê¸°ëŠ¥**:
- âœ… **SQLite WAL ëª¨ë“œ**: ë™ì‹œ ì½ê¸°/ì“°ê¸° ê°€ëŠ¥
- âœ… **Connection Pooling**: `StaticPool` (ë‹¨ì¼ ì—°ê²° ìœ ì§€)
- âœ… **íƒ€ì„ì•„ì›ƒ ì„¤ì •**: 30ì´ˆ lock timeout
- âœ… **ì„±ëŠ¥ ìµœì í™”**: 10MB ìºì‹œ, foreign key ì œì•½

**êµ¬í˜„ ë°©ì‹**:
```python
engine = create_engine(
    DATABASE_URL,
    connect_args={
        "check_same_thread": False,  # ë‹¤ì¤‘ ìŠ¤ë ˆë“œ í—ˆìš©
        "timeout": 30  # 30ì´ˆ ëŒ€ê¸°
    },
    poolclass=StaticPool,  # SQLiteì— ìµœì í™”
    pool_pre_ping=True,  # ì—°ê²° ê²€ì¦
)

@event.listens_for(engine, "connect")
def set_sqlite_pragma(dbapi_conn, connection_record):
    cursor.execute("PRAGMA journal_mode=WAL")  # ë™ì‹œ ì½ê¸°/ì“°ê¸°
    cursor.execute("PRAGMA cache_size=-10000")  # 10MB ìºì‹œ
```

**ì¥ì **:
- WAL ëª¨ë“œ: ì—¬ëŸ¬ reader + 1 writer ë™ì‹œ ì‘ë™ ê°€ëŠ¥
- Lock contention ê°ì†Œ
- ì„±ëŠ¥: ìºì‹œë¡œ ë””ìŠ¤í¬ I/O ê°ì†Œ

#### 4. **ë¹„ë™ê¸° íŒŒì¼ I/O (Async File Operations)**
**êµ¬í˜„ íŒŒì¼**: `backend/app/api/routes.py`

**í˜„ì¬ ê¸°ëŠ¥**:
- âœ… **aiofiles ì‚¬ìš©**: ëª¨ë“  íŒŒì¼ ì½ê¸°/ì“°ê¸° ë¹„ë™ê¸°
- âœ… **Non-blocking**: Event loop ì°¨ë‹¨ ë°©ì§€
- âœ… **ë™ì‹œ íŒŒì¼ ì‘ì—…**: ì—¬ëŸ¬ ì‚¬ìš©ìê°€ ê°ì íŒŒì¼ ì‘ì—… ê°€ëŠ¥

**êµ¬í˜„ ë°©ì‹**:
```python
async def write_artifact_to_workspace(artifact: dict) -> dict:
    import aiofiles
    async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
        await f.write(content)
```

**ì¥ì **:
- CPU íš¨ìœ¨: I/O ëŒ€ê¸° ì¤‘ ë‹¤ë¥¸ ìš”ì²­ ì²˜ë¦¬
- í™•ì¥ì„±: ìˆ˜ë°± ê°œì˜ ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ê°€ëŠ¥

---

## âš ï¸ í˜„ì¬ í•œê³„ì  ë° ê°œì„  í•„ìš” ì‚¬í•­

### ğŸ”´ Critical Issues

#### 1. **ë©”ëª¨ë¦¬ ë‚´ ì„¸ì…˜ ì €ì¥ì†Œ (In-Memory SessionStore)**
**ë¬¸ì œì **:
- ì„œë²„ ì¬ì‹œì‘ ì‹œ ëª¨ë“  ì„¸ì…˜ ë°ì´í„° ì†ì‹¤
- ìˆ˜í‰ í™•ì¥ ë¶ˆê°€ëŠ¥ (ì—¬ëŸ¬ ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ ì‹œ ì„¸ì…˜ ê³µìœ  ì•ˆ ë¨)
- ë¬´í•œì • ë©”ëª¨ë¦¬ ì¦ê°€ (ì„¸ì…˜ ë§Œë£Œ ë¡œì§ ì—†ìŒ)

**í˜„ì¬ ì½”ë“œ**:
```python
class SessionStore:
    def __init__(self):
        self._frameworks: Dict[str, FrameworkType] = {}  # ë©”ëª¨ë¦¬ì—ë§Œ ì €ì¥
        self._workspaces: Dict[str, str] = {}
```

**ê¶Œì¥ í•´ê²°ì±…**:
```python
# Option 1: Redis ê¸°ë°˜ ì„¸ì…˜ ì €ì¥ì†Œ
import aioredis

class RedisSessionStore:
    def __init__(self):
        self.redis = aioredis.from_url("redis://localhost")

    async def get_framework(self, session_id: str) -> FrameworkType:
        value = await self.redis.get(f"session:{session_id}:framework")
        return value.decode() if value else "standard"

    async def set_framework(self, session_id: str, framework: FrameworkType):
        await self.redis.setex(
            f"session:{session_id}:framework",
            3600 * 24,  # 24ì‹œê°„ TTL
            framework
        )

# Option 2: DB ê¸°ë°˜ ì„¸ì…˜ ì €ì¥ì†Œ
# SQLite â†’ PostgreSQL/MySQLë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
```

**ì˜í–¥**:
- âœ… ì„œë²„ ì¬ì‹œì‘í•´ë„ ì„¸ì…˜ ìœ ì§€
- âœ… ìˆ˜í‰ í™•ì¥ ê°€ëŠ¥ (ë¡œë“œ ë°¸ëŸ°ì„œ + ì—¬ëŸ¬ ì„œë²„)
- âœ… ìë™ ë§Œë£Œ (TTL)

---

#### 2. **ì›Œí¬í”Œë¡œìš° ìºì‹œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ (Workflow Cache Memory Leak)**
**ë¬¸ì œì **:
- `_deepagent_workflows` ë”•ì…”ë„ˆë¦¬ê°€ ë¬´í•œì • ì¦ê°€
- ì„¸ì…˜ ì¢…ë£Œí•´ë„ ì›Œí¬í”Œë¡œìš° ê°ì²´ ì‚­ì œ ì•ˆ ë¨
- LLM ëª¨ë¸ì´ ë©”ëª¨ë¦¬ì— ê³„ì† ìœ ì§€

**í˜„ì¬ ì½”ë“œ** (`workflow_service.py`):
```python
class WorkflowService:
    def __init__(self):
        self._deepagent_workflows: Dict[str, Any] = {}  # ë¬´í•œ ì¦ê°€
```

**ê¶Œì¥ í•´ê²°ì±…**:
```python
from datetime import datetime, timedelta
from collections import OrderedDict

class WorkflowService:
    def __init__(self):
        self._deepagent_workflows: OrderedDict[str, Dict] = OrderedDict()
        self._max_cache_size = 100  # ìµœëŒ€ 100ê°œ ì„¸ì…˜
        self._cache_ttl = timedelta(hours=1)  # 1ì‹œê°„ í›„ ë§Œë£Œ

    async def get_workflow(self, session_id: str, workspace: str):
        # LRU eviction: ì˜¤ë˜ëœ ê²ƒë¶€í„° ì œê±°
        if len(self._deepagent_workflows) >= self._max_cache_size:
            oldest_session = next(iter(self._deepagent_workflows))
            del self._deepagent_workflows[oldest_session]
            logger.info(f"Evicted oldest workflow: {oldest_session}")

        # TTL ì²´í¬
        if session_id in self._deepagent_workflows:
            cache_entry = self._deepagent_workflows[session_id]
            if datetime.now() - cache_entry['created_at'] > self._cache_ttl:
                del self._deepagent_workflows[session_id]
                logger.info(f"Expired workflow: {session_id}")

        # ì›Œí¬í”Œë¡œìš° ìƒì„± ë˜ëŠ” ì¬ì‚¬ìš©
        if session_id not in self._deepagent_workflows:
            workflow = await self._create_deepagent_workflow(...)
            self._deepagent_workflows[session_id] = {
                'workflow': workflow,
                'created_at': datetime.now()
            }

        return self._deepagent_workflows[session_id]['workflow']
```

**ì˜í–¥**:
- âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ
- âœ… ì˜¤ë˜ëœ ì„¸ì…˜ ìë™ ì •ë¦¬
- âœ… ì„œë²„ ì•ˆì •ì„± í–¥ìƒ

---

#### 3. **ìš”ì²­ ìŠ¤ì¼€ì¤„ë§ ë° íì‰ ì—†ìŒ (No Request Queuing)**
**ë¬¸ì œì **:
- ë™ì‹œì— 100ëª… ì‚¬ìš©ìê°€ workflow ì‹¤í–‰ ì‹œ 100ê°œ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
- GPU/ë©”ëª¨ë¦¬ ê³ ê°ˆ
- ì„œë²„ í¬ë˜ì‹œ ê°€ëŠ¥ì„±

**í˜„ì¬ ì½”ë“œ**:
```python
# routes.py - ì œí•œ ì—†ì´ ì¦‰ì‹œ ì‹¤í–‰
@router.post("/workflow/execute")
async def execute_workflow(request: ChatRequest):
    workflow = await workflow_service.get_workflow(...)  # ì¦‰ì‹œ ìƒì„±
    async for update in workflow.execute_stream(...):  # ì¦‰ì‹œ ì‹¤í–‰
        yield update
```

**ê¶Œì¥ í•´ê²°ì±…**:
```python
import asyncio
from collections import deque

class WorkflowQueue:
    def __init__(self, max_concurrent=10):
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.queue = deque()
        self.active_count = 0

    async def enqueue(self, session_id: str, workflow_fn):
        """íì— ì›Œí¬í”Œë¡œìš° ì¶”ê°€"""
        queue_position = len(self.queue) + self.active_count + 1

        # í ëŒ€ê¸° ìƒíƒœ ì „ì†¡
        yield {
            "type": "queued",
            "position": queue_position,
            "estimated_wait_seconds": queue_position * 60  # 1ë¶„/ì‘ì—… ì˜ˆìƒ
        }

        # Semaphoreë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ
        async with self.semaphore:
            self.active_count += 1
            try:
                yield {"type": "started", "message": "ì›Œí¬í”Œë¡œìš° ì‹œì‘"}
                async for update in workflow_fn():
                    yield update
            finally:
                self.active_count -= 1

# ì‚¬ìš©
workflow_queue = WorkflowQueue(max_concurrent=10)

@router.post("/workflow/execute")
async def execute_workflow(request: ChatRequest):
    async def workflow_fn():
        workflow = await workflow_service.get_workflow(...)
        async for update in workflow.execute_stream(...):
            yield update

    # í í†µê³¼ í›„ ì‹¤í–‰
    async for update in workflow_queue.enqueue(request.session_id, workflow_fn):
        yield json.dumps(update) + "\n"
```

**íš¨ê³¼**:
- âœ… ë™ì‹œ ì‹¤í–‰ ì œí•œ (ì˜ˆ: ìµœëŒ€ 10ê°œ)
- âœ… ë‚˜ë¨¸ì§€ëŠ” íì—ì„œ ëŒ€ê¸°
- âœ… GPU/ë©”ëª¨ë¦¬ ì•ˆì •ì  ì‚¬ìš©
- âœ… ì‚¬ìš©ìì—ê²Œ ëŒ€ê¸° ì‹œê°„ ì•Œë¦¼

---

#### 4. **Rate Limiting ì—†ìŒ (No Rate Limiting)**
**ë¬¸ì œì **:
- í•œ ì‚¬ìš©ìê°€ 1ì´ˆì— 100ë²ˆ ìš”ì²­ ê°€ëŠ¥
- DDoS ê³µê²©ì— ì·¨ì•½
- ì •ìƒ ì‚¬ìš©ì ì„œë¹„ìŠ¤ ë°©í•´

**ê¶Œì¥ í•´ê²°ì±…**:
```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@router.post("/workflow/execute")
@limiter.limit("10/minute")  # ë¶„ë‹¹ 10íšŒ ì œí•œ
async def execute_workflow(request: Request, chat_request: ChatRequest):
    ...
```

**íš¨ê³¼**:
- âœ… ì‚¬ìš©ìë‹¹ ìš”ì²­ ì œí•œ
- âœ… ì„œë²„ ë¦¬ì†ŒìŠ¤ ê³µì • ë¶„ë°°
- âœ… ì•…ì˜ì  ì‚¬ìš© ë°©ì§€

---

### ğŸŸ¡ Medium Priority Issues

#### 5. **ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ ì—†ìŒ (No Session Timeout)**
**ë¬¸ì œì **:
- ì‚¬ìš©ìê°€ ë¸Œë¼ìš°ì € ë‹«ì•„ë„ ì„¸ì…˜ ìœ ì§€
- ë©”ëª¨ë¦¬ ëˆ„ì 

**í•´ê²°ì±…**:
```python
class SessionStore:
    def __init__(self):
        self._last_access: Dict[str, datetime] = {}
        self.session_timeout = timedelta(hours=2)

    async def cleanup_expired_sessions(self):
        """ì£¼ê¸°ì ìœ¼ë¡œ ë§Œë£Œëœ ì„¸ì…˜ ì‚­ì œ"""
        while True:
            await asyncio.sleep(300)  # 5ë¶„ë§ˆë‹¤ ì²´í¬
            now = datetime.now()
            expired = [
                sid for sid, last_time in self._last_access.items()
                if now - last_time > self.session_timeout
            ]
            for sid in expired:
                await self.delete_session(sid)
                logger.info(f"Auto-deleted expired session: {sid}")
```

---

#### 6. **SQLite í™•ì¥ì„± í•œê³„ (SQLite Scalability)**
**ë¬¸ì œì **:
- SQLiteëŠ” ë‹¨ì¼ ì„œë²„ìš©
- ìˆ˜í‰ í™•ì¥ ë¶ˆê°€ëŠ¥
- 1000+ ë™ì‹œ ì‚¬ìš©ì ì‹œ ë³‘ëª©

**í•´ê²°ì±…**:
```python
# PostgreSQLë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
DATABASE_URL = "postgresql+asyncpg://user:pass@localhost/dbname"

engine = create_async_engine(
    DATABASE_URL,
    pool_size=20,  # ì—°ê²° í’€
    max_overflow=10,
    pool_pre_ping=True
)
```

**íš¨ê³¼**:
- âœ… ì§„ì •í•œ ë™ì‹œ ì“°ê¸°
- âœ… í´ëŸ¬ìŠ¤í„°ë§ ê°€ëŠ¥
- âœ… 10,000+ ë™ì‹œ ì‚¬ìš©ì ì§€ì›

---

## ğŸ¯ ê¶Œì¥ ì•„í‚¤í…ì²˜ (Recommended Architecture)

### Phase 1: ì¦‰ì‹œ ì ìš© ê°€ëŠ¥ (Quick Wins)
```
1. Workflow Cache LRU + TTL ì¶”ê°€
2. Request Queue + Semaphore (max_concurrent=10)
3. Rate Limiting (slowapi)
4. Session Timeout ìë™ ì •ë¦¬
```

**ì˜ˆìƒ íš¨ê³¼**:
- í˜„ì¬ ì‹œìŠ¤í…œì—ì„œ 10-50ëª… ë™ì‹œ ì‚¬ìš©ì ì•ˆì •ì  ì²˜ë¦¬

---

### Phase 2: ì¤‘ê¸° ê°œì„  (Medium Term)
```
1. Redis Session Store
2. PostgreSQL ë§ˆì´ê·¸ë ˆì´ì…˜
3. Celery ì›Œí¬ í (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)
4. Nginx + Gunicorn (í”„ë¡œë•ì…˜ ë°°í¬)
```

**ì˜ˆìƒ íš¨ê³¼**:
- 50-500ëª… ë™ì‹œ ì‚¬ìš©ì
- ì„œë²„ ì¬ì‹œì‘í•´ë„ ì„¸ì…˜ ìœ ì§€
- ìˆ˜í‰ í™•ì¥ ê°€ëŠ¥

---

### Phase 3: ëŒ€ê·œëª¨ í™•ì¥ (Large Scale)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Load Balancer (Nginx)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI #1  â”‚    â”‚  FastAPI #2  â”‚  (Auto-scaling)
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Redis Cluster    â”‚  (Session Store)
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL   â”‚    â”‚    Celery    â”‚  (Background Tasks)
â”‚  (Primary)   â”‚    â”‚   Workers    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL   â”‚
â”‚  (Replica)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ì˜ˆìƒ íš¨ê³¼**:
- 1000+ ë™ì‹œ ì‚¬ìš©ì
- 99.9% uptime
- Auto-scaling

---

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì˜ˆìƒì¹˜

| êµ¬í˜„ ë‹¨ê³„ | ë™ì‹œ ì‚¬ìš©ì | í‰ê·  ì‘ë‹µì‹œê°„ | ë©”ëª¨ë¦¬ ì‚¬ìš© | ë¹„ìš©/ì›” |
|---------|----------|------------|----------|---------|
| **í˜„ì¬** | ~10ëª… | 2-5ì´ˆ | 2-4GB | $50 |
| **Phase 1** | ~50ëª… | 3-8ì´ˆ | 4-8GB | $50 |
| **Phase 2** | ~500ëª… | 5-15ì´ˆ | 8-16GB | $200 |
| **Phase 3** | 1000+ëª… | 10-30ì´ˆ | 16-64GB | $500+ |

---

## ğŸ› ï¸ êµ¬í˜„ ìš°ì„ ìˆœìœ„

### ğŸ”¥ High Priority (1ì£¼ì¼)
1. âœ… **Workflow Cache LRU**: ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€
2. âœ… **Request Semaphore**: GPU ê³¼ë¶€í•˜ ë°©ì§€
3. âœ… **Rate Limiting**: DDoS ë°©ì§€

### ğŸŸ¡ Medium Priority (1ê°œì›”)
4. â³ **Redis Session Store**: ì„¸ì…˜ ì˜ì†ì„±
5. â³ **Session Timeout**: ìë™ ì •ë¦¬
6. â³ **PostgreSQL**: DB í™•ì¥ì„±

### ğŸŸ¢ Low Priority (3ê°œì›”)
7. â³ **Celery Queue**: ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
8. â³ **Load Balancer**: ìˆ˜í‰ í™•ì¥
9. â³ **Monitoring**: Prometheus + Grafana

---

## ğŸ’¡ ê²°ë¡ 

**í˜„ì¬ ì‹œìŠ¤í…œì€**:
- âœ… ê¸°ë³¸ì ì¸ ë‹¤ì¤‘ ì‚¬ìš©ì ì§€ì› (10ëª… ì´í•˜)
- âœ… Thread-safe ì„¸ì…˜ ê´€ë¦¬
- âœ… ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ê²©ë¦¬
- âš ï¸ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ìœ„í—˜
- âš ï¸ í™•ì¥ì„± ì œí•œ
- âŒ Rate limiting ì—†ìŒ

**ì¦‰ì‹œ ê°œì„  í•„ìš”**:
1. Workflow Cache ê´€ë¦¬ (LRU + TTL)
2. Request Queue (Semaphore)
3. Rate Limiting

**ì´ë ‡ê²Œ í•˜ë©´**:
- í˜„ì¬ 10ëª… â†’ 50-100ëª… ì•ˆì •ì  ì²˜ë¦¬ ê°€ëŠ¥
- ì„œë²„ í¬ë˜ì‹œ ë°©ì§€
- ë©”ëª¨ë¦¬ ì•ˆì •í™”
