# LLM ëª¨ë¸ ë³€ê²½ ê°œì„  ê³„íš

## ğŸ“‹ ê°œìš”

í˜„ì¬ Agentic CoderëŠ” ë‘ ê°œì˜ LLM ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:
- **Reasoning Model**: DeepSeek-R1 (ë¶„ì„, ê³„íš ìˆ˜ë¦½)
- **Coding Model**: Qwen-Coder (ì½”ë“œ ìƒì„±, ë¦¬ë·°)

ë‹¨ì¼ ëª¨ë¸(ì˜ˆ: `gpt-oss-120b`)ë¡œ ë³€ê²½í•  ê²½ìš°ì˜ ì˜í–¥ë„ì™€ ê°œì„  ë°©ì•ˆì„ ì •ë¦¬í•©ë‹ˆë‹¤.

---

## ğŸ” í˜„ì¬ êµ¬ì¡° ë¶„ì„

### 1. ì„¤ì • êµ¬ì¡° (`backend/app/core/config.py`)

```python
class Settings(BaseSettings):
    # í˜„ì¬: ë‘ ê°œì˜ ì—”ë“œí¬ì¸íŠ¸
    vllm_reasoning_endpoint: str = "http://localhost:8001/v1"
    vllm_coding_endpoint: str = "http://localhost:8002/v1"

    # í˜„ì¬: ë‘ ê°œì˜ ëª¨ë¸
    reasoning_model: str = "deepseek-ai/DeepSeek-R1"
    coding_model: str = "Qwen/Qwen3-8B-Coder"
```

### 2. ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ (`shared/prompts/`)

| íŒŒì¼ | ëª¨ë¸ | íŠ¹ì§• |
|------|------|------|
| `deepseek_r1.py` | DeepSeek-R1 | `<think></think>` íƒœê·¸ í•„ìˆ˜ |
| `qwen_coder.py` | Qwen-Coder | ì½”ë“œ ë¸”ë¡ ì¤‘ì‹¬, ë‚®ì€ temperature |

### 3. ëª¨ë¸ ì‚¬ìš©ì²˜

| ì»´í¬ë„ŒíŠ¸ | ì‚¬ìš© ëª¨ë¸ | íŒŒì¼ ìœ„ì¹˜ |
|----------|----------|-----------|
| Supervisor | DeepSeek-R1 | `backend/core/supervisor.py` |
| Coder | Qwen-Coder | `backend/app/agent/langgraph/nodes/coder.py` |
| Reviewer | Qwen-Coder | `backend/app/agent/langgraph/nodes/reviewer.py` |
| Refiner | (ì—†ìŒ - ì‹œë®¬ë ˆì´ì…˜) | `backend/app/agent/langgraph/nodes/refiner.py` |

---

## â“ í•µì‹¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€

### Q1: ë‹¨ì¼ ëª¨ë¸ ì‚¬ìš© ì‹œ ë³€ê²½ì‚¬í•­

ë‹¨ì¼ ëª¨ë¸(ì˜ˆ: `gpt-oss-120b`)ë¡œ ë³€ê²½ ì‹œ:

| í•­ëª© | í˜„ì¬ | ë³€ê²½ í›„ | ì˜í–¥ë„ |
|------|------|---------|--------|
| ì—”ë“œí¬ì¸íŠ¸ | 2ê°œ | 1ê°œ | ğŸŸ¢ ë‚®ìŒ |
| ëª¨ë¸ ì„¤ì • | 2ê°œ | 1ê°œ | ğŸŸ¢ ë‚®ìŒ |
| í”„ë¡¬í”„íŠ¸ | ëª¨ë¸ë³„ ë¶„ë¦¬ | **í†µí•© í•„ìš”** | ğŸ”´ ë†’ìŒ |
| íŒŒë¼ë¯¸í„° | ëª¨ë¸ë³„ ìµœì í™” | **ì¬ì¡°ì • í•„ìš”** | ğŸŸ¡ ì¤‘ê°„ |

### Q2: System Prompt ë³€ê²½ í•„ìš”ì„±

**ì˜ˆ, ë³€ê²½ì´ í•„ìš”í•©ë‹ˆë‹¤.**

í˜„ì¬ í”„ë¡¬í”„íŠ¸ì˜ ëª¨ë¸ íŠ¹í™” ìš”ì†Œ:

```python
# DeepSeek-R1 ì „ìš© (ë‹¤ë¥¸ ëª¨ë¸ì—ì„œ ì‘ë™ ì•ˆ í•¨)
DEEPSEEK_R1_SYSTEM_PROMPT = """
CRITICAL CONSTRAINTS:
1. ALWAYS use <think></think> tags...  # âŒ GPT ê³„ì—´ì—ì„œ ë¯¸ì§€ì›
"""

# Qwen-Coder ì „ìš©
QWEN_CODER_CONFIG = {
    "stop": ["</code>", "```\n\n"],  # ëª¨ë¸ë³„ ì¢…ë£Œ í† í° ë‹¤ë¦„
}
```

**í•´ê²° ë°©ì•ˆ: ëª¨ë¸ ì¶”ìƒí™” ê³„ì¸µ ë„ì…**

### Q3: DeepAgent êµ¬ì¡° ë³€ê²½ í•„ìš”ì„±

**ë¶€ë¶„ì  ë³€ê²½ í•„ìš”:**

| ì˜ì—­ | ë³€ê²½ í•„ìš” | ì´ìœ  |
|------|----------|------|
| ì›Œí¬í”Œë¡œìš° (LangGraph) | âŒ ë¶ˆí•„ìš” | ëª¨ë¸ ë…ë¦½ì  ì„¤ê³„ |
| ë…¸ë“œ (Coder, Reviewer) | âš ï¸ ìˆ˜ì • í•„ìš” | ì—”ë“œí¬ì¸íŠ¸/í”„ë¡¬í”„íŠ¸ ì°¸ì¡° ë³€ê²½ |
| í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ | âœ… ì¬ì„¤ê³„ í•„ìš” | ëª¨ë¸ë³„ ì–´ëŒ‘í„° íŒ¨í„´ ì ìš© |
| ì„¤ì • (Config) | âœ… ì¬ì„¤ê³„ í•„ìš” | ë‹¨ì¼ ëª¨ë¸ ì§€ì› êµ¬ì¡°ë¡œ |

---

## ğŸ¯ ê°œì„  ë°©í–¥

### Phase 1: ëª¨ë¸ ì¶”ìƒí™” ê³„ì¸µ ë„ì… (ìš°ì„ ìˆœìœ„: ë†’ìŒ)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Application Layer                        â”‚
â”‚  (Supervisor, Coder, Reviewer, Refiner, etc.)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LLM Provider Interface                      â”‚
â”‚  - generate(prompt, task_type)                              â”‚
â”‚  - stream(prompt, task_type)                                â”‚
â”‚  - get_config(task_type)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DeepSeek R1 â”‚  â”‚ Qwen Coder  â”‚  â”‚  GPT-OSS    â”‚
â”‚   Adapter   â”‚  â”‚   Adapter   â”‚  â”‚   Adapter   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 2: í†µí•© í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ

```python
# ìƒˆë¡œìš´ êµ¬ì¡°: shared/prompts/base.py
class BasePromptTemplate:
    """ëª¨ë¸ ë…ë¦½ì  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""

    @abstractmethod
    def get_system_prompt(self) -> str:
        pass

    @abstractmethod
    def format_reasoning(self, content: str) -> str:
        """ëª¨ë¸ì— ë§ëŠ” reasoning í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
        pass

    @abstractmethod
    def parse_response(self, response: str) -> Dict:
        """ëª¨ë¸ ì‘ë‹µì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ íŒŒì‹±"""
        pass


# ëª¨ë¸ë³„ êµ¬í˜„
class DeepSeekAdapter(BasePromptTemplate):
    def format_reasoning(self, content: str) -> str:
        return f"<think>{content}</think>"

class GPTAdapter(BasePromptTemplate):
    def format_reasoning(self, content: str) -> str:
        return f"Let me think step by step:\n{content}"
```

### Phase 3: ì„¤ì • í†µí•©

```python
# ê°œì„ ëœ config.py
class Settings(BaseSettings):
    # í†µí•© ì—”ë“œí¬ì¸íŠ¸ (ë˜ëŠ” ì—¬ëŸ¬ ê°œ ì§€ì›)
    llm_endpoint: str = "http://localhost:8001/v1"

    # ë‹¨ì¼ ëª¨ë¸ ë˜ëŠ” íƒœìŠ¤í¬ë³„ ëª¨ë¸ ì§€ì •
    llm_model: str = "gpt-oss-120b"

    # ì„ íƒì : íƒœìŠ¤í¬ë³„ ëª¨ë¸ ì˜¤ë²„ë¼ì´ë“œ
    reasoning_model: Optional[str] = None  # Noneì´ë©´ llm_model ì‚¬ìš©
    coding_model: Optional[str] = None     # Noneì´ë©´ llm_model ì‚¬ìš©

    # ëª¨ë¸ íƒ€ì… (í”„ë¡¬í”„íŠ¸ ì–´ëŒ‘í„° ì„ íƒìš©)
    model_type: Literal["deepseek", "qwen", "gpt", "claude", "generic"] = "generic"
```

---

## ğŸ“ êµ¬í˜„ ê³„íš

### Stage 1: ê¸°ë°˜ êµ¬ì¡° êµ¬í˜„ âœ… ì™„ë£Œ

- [x] `shared/llm/base.py` - LLM Provider ì¸í„°í˜ì´ìŠ¤ ì •ì˜
  - `BaseLLMProvider` ì¶”ìƒ í´ë˜ìŠ¤
  - `LLMConfig`, `LLMResponse` ë°ì´í„° í´ë˜ìŠ¤
  - `TaskType` enum (REASONING, CODING, REVIEW, REFINE, GENERAL)
  - `LLMProviderFactory` íŒ©í† ë¦¬ íŒ¨í„´
- [x] `shared/llm/adapters/` - ëª¨ë¸ë³„ ì–´ëŒ‘í„° êµ¬í˜„
  - [x] `deepseek_adapter.py` - `<think>` íƒœê·¸ ì§€ì›
  - [x] `qwen_adapter.py` - ì½”ë”© íŠ¹í™” ì„¤ì •
  - [x] `generic_adapter.py` - GPT, Claude, Llama, Mistral ì§€ì›
- [x] `backend/app/core/config.py` - ì„¤ì • êµ¬ì¡° í™•ì¥
  - `model_type`, `llm_endpoint`, `llm_model` ì¶”ê°€
  - `get_reasoning_endpoint`, `get_coding_endpoint` í”„ë¡œí¼í‹°

### Stage 2: ë…¸ë“œ ë¦¬íŒ©í† ë§ âœ… ì™„ë£Œ

- [x] `coder.py` - LLM Provider ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½
  - `_get_code_generation_prompt()` ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ ì„ íƒ
- [x] `reviewer.py` - LLM Provider ì–´ëŒ‘í„° ì ìš©
  - `LLMProviderFactory.create()` ì‚¬ìš©
- [x] `refiner.py` - ì‹¤ì œ LLM í˜¸ì¶œ êµ¬í˜„
  - `_apply_fix_with_llm()` í•¨ìˆ˜ ì¶”ê°€
  - Fallback to heuristic ì§€ì›
- [ ] `supervisor.py` - ì–´ëŒ‘í„° íŒ¨í„´ ì ìš© (ì„ íƒì )

### Stage 3: í”„ë¡¬í”„íŠ¸ í†µí•© âœ… ì™„ë£Œ

- [x] `shared/prompts/generic.py` - ë²”ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
- [x] ì–´ëŒ‘í„° ë‚´ í†µí•© í”„ë¡¬í”„íŠ¸
  - ê° ì–´ëŒ‘í„°ì— `SYSTEM_PROMPTS` ë”•ì…”ë„ˆë¦¬ í¬í•¨
  - `format_prompt()`, `format_system_prompt()` ë©”ì„œë“œ

### Stage 4: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ âœ… ì™„ë£Œ

- [x] ëª¨ë“ˆ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸
- [x] ë‹¨ì¼ ëª¨ë¸ ëª¨ë“œ í…ŒìŠ¤íŠ¸ (`TestSingleModelMode` - 5 tests passed)
- [x] ë©€í‹° ëª¨ë¸ ëª¨ë“œ í…ŒìŠ¤íŠ¸ (`TestMultiModelMode` - 6 tests passed)
- [x] Fallback ë™ì‘ ê²€ì¦ (`TestFallbackBehavior`, `TestRefinerFallback` - 7 tests passed)
- [x] Config í†µí•© í…ŒìŠ¤íŠ¸ (`TestConfigIntegration` - 3 tests passed)
- [x] Async ì‘ì—… í…ŒìŠ¤íŠ¸ (`TestAsyncOperations` - 2 tests passed)

**í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼: 28 passed, 0 failed**

í…ŒìŠ¤íŠ¸ íŒŒì¼: `backend/tests/integration/test_llm_provider.py`

---

## ğŸ”§ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ë¹ ë¥¸ ë³€ê²½

ë‹¨ì¼ ëª¨ë¸ë¡œ ë¹ ë¥´ê²Œ ì „í™˜í•˜ë ¤ë©´ ì•„ë˜ ìµœì†Œ ë³€ê²½ë§Œ ìˆ˜í–‰:

### 1. Config ìˆ˜ì •

```python
# backend/app/core/config.py
class Settings(BaseSettings):
    # ê¸°ì¡´ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜)
    vllm_reasoning_endpoint: str = "http://localhost:8001/v1"
    vllm_coding_endpoint: str = "http://localhost:8001/v1"  # ë™ì¼ ì—”ë“œí¬ì¸íŠ¸

    # ë‹¨ì¼ ëª¨ë¸ ì‚¬ìš©
    reasoning_model: str = "gpt-oss-120b"
    coding_model: str = "gpt-oss-120b"  # ë™ì¼ ëª¨ë¸

    # ëª¨ë¸ íƒ€ì… ì¶”ê°€ (í”„ë¡¬í”„íŠ¸ ì„ íƒìš©)
    model_type: str = "generic"
```

### 2. ë²”ìš© í”„ë¡¬í”„íŠ¸ ì¶”ê°€

```python
# shared/prompts/generic.py
GENERIC_SYSTEM_PROMPT = """You are an AI assistant specialized in software engineering.

For complex tasks, think through the problem step by step before providing a solution.
Provide clear, executable code with proper error handling.
"""
```

### 3. ë…¸ë“œì—ì„œ í”„ë¡¬í”„íŠ¸ ë¶„ê¸°

```python
# coder.py ë“±ì—ì„œ
if settings.model_type == "deepseek":
    system_prompt = DEEPSEEK_R1_SYSTEM_PROMPT
elif settings.model_type == "qwen":
    system_prompt = QWEN_CODER_SYSTEM_PROMPT
else:
    system_prompt = GENERIC_SYSTEM_PROMPT
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **Reasoning í’ˆì§ˆ**: DeepSeek-R1ì˜ `<think>` íƒœê·¸ëŠ” reasoning í’ˆì§ˆ í–¥ìƒì— ê¸°ì—¬. ë‹¤ë¥¸ ëª¨ë¸ì—ì„œëŠ” "step-by-step" í”„ë¡¬í”„íŒ…ìœ¼ë¡œ ëŒ€ì²´ í•„ìš”.

2. **ì½”ë“œ ìƒì„± í’ˆì§ˆ**: ì½”ë”© íŠ¹í™” ëª¨ë¸(Qwen-Coder)ì—ì„œ ë²”ìš© ëª¨ë¸ë¡œ ì „í™˜ ì‹œ í’ˆì§ˆ ì €í•˜ ê°€ëŠ¥. í”„ë¡¬í”„íŠ¸ ìµœì í™” í•„ìš”.

3. **ë¹„ìš©/ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„**: ë‹¨ì¼ ëŒ€í˜• ëª¨ë¸ vs íŠ¹í™” ì†Œí˜• ëª¨ë¸ ë¹„êµ í•„ìš”.

4. **Fallback ì „ëµ**: ëª¨ë¸ ì¥ì•  ì‹œ ë‹¤ë¥¸ ëª¨ë¸ë¡œ ìë™ ì „í™˜ ë¡œì§ ê³ ë ¤.

---

## ğŸ“Š ì˜ˆìƒ íš¨ê³¼

| ì§€í‘œ | í˜„ì¬ (2 ëª¨ë¸) | ë³€ê²½ í›„ (1 ëª¨ë¸) |
|------|--------------|-----------------|
| ë°°í¬ ë³µì¡ë„ | ë†’ìŒ (2 ì„œë²„) | ë‚®ìŒ (1 ì„œë²„) |
| ìœ ì§€ë³´ìˆ˜ | ë³µì¡ | ë‹¨ìˆœ |
| ë¹„ìš© | ëª¨ë¸ë³„ ìƒì´ | í†µí•© ê°€ëŠ¥ |
| íŠ¹í™” ì„±ëŠ¥ | ë†’ìŒ | ë‹¤ì†Œ ë‚®ì„ ìˆ˜ ìˆìŒ |

---

## ğŸ—“ï¸ íƒ€ì„ë¼ì¸

| ë‹¨ê³„ | ì‘ì—… | ì˜ˆìƒ ì†Œìš” |
|------|------|----------|
| ì¦‰ì‹œ ì ìš© | ì„¤ì • ë³€ê²½ + ë²”ìš© í”„ë¡¬í”„íŠ¸ | 1ì‹œê°„ |
| Stage 1 | ê¸°ë°˜ êµ¬ì¡° | 1-2ì¼ |
| Stage 2 | ë…¸ë“œ ë¦¬íŒ©í† ë§ | 2-3ì¼ |
| Stage 3 | í”„ë¡¬í”„íŠ¸ í†µí•© | 1-2ì¼ |
| Stage 4 | í…ŒìŠ¤íŠ¸ | 1ì¼ |
| **Total** | **ì™„ì „í•œ ì¶”ìƒí™”** | **5-8ì¼** |

---

*Last Updated: 2026-01-05*
*Author: AI Assistant*
*Implementation Status: Stage 1-4 Complete âœ…*
