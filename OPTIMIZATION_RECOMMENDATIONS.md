# H100 GPU ìµœì í™” ê¶Œì¥ì‚¬í•­

## í˜„ì¬ í•˜ë“œì›¨ì–´ êµ¬ì„±
- **GPU 0**: NVIDIA H100 96GB NVL (DeepSeek-R1)
- **GPU 1**: NVIDIA H100 96GB NVL (Qwen3-Coder)

## ìµœì í™” ì „ëµ

### 1. ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜ ì¦ê°€

#### í˜„ì¬ ì„¤ì •
```python
max_parallel_agents = 10  # workflow_manager.py:531
```

#### ê¶Œì¥ ì„¤ì •
```python
max_parallel_agents = 25  # H100 + vLLMì˜ ì„±ëŠ¥ì„ ê³ ë ¤
```

**ì˜ˆìƒ íš¨ê³¼:**
- ì½”ë”© ì‘ì—… ì²˜ë¦¬ ì†ë„ 2-2.5ë°° í–¥ìƒ
- GPU 1 í™œìš©ë„ ì¦ê°€ (40% â†’ 80%+)
- ì „ì²´ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ ì‹œê°„ 30-40% ë‹¨ì¶•

**ì ìš© ë°©ë²•:**
```bash
# backend/app/agent/langchain/workflow_manager.py ìˆ˜ì •
sed -i 's/self.max_parallel_agents = 10/self.max_parallel_agents = 25/' \
  backend/app/agent/langchain/workflow_manager.py
```

---

### 2. ì„¸ì…˜ ê°„ íŒŒì´í”„ë¼ì´ë‹

#### ê°œë…
ì—¬ëŸ¬ ì‚¬ìš©ì ì„¸ì…˜ì´ ë™ì‹œì— ì‹¤í–‰ë  ë•Œ GPU í™œìš©ë„ ê·¹ëŒ€í™”:

```
Session A: [Planning@GPU0] â†’ [Coding@GPU1] â†’ [Review@GPU1]
Session B:                    [Planning@GPU0] â†’ [Coding@GPU1] â†’ [Review@GPU1]
                                            â†‘ ì´ ì‹œì ì— GPU0 ìœ íœ´
```

#### êµ¬í˜„ ë°©ì•ˆ

**Option A: ë‹¤ì¤‘ ì„¸ì…˜ ìë™ íŒŒì´í”„ë¼ì´ë‹**
```python
# ìƒˆë¡œìš´ SessionScheduler ì¶”ê°€
class SessionScheduler:
    def __init__(self):
        self.gpu0_queue = asyncio.Queue()  # DeepSeek-R1 ì‘ì—…
        self.gpu1_queue = asyncio.Queue()  # Qwen3-Coder ì‘ì—…

    async def schedule_planning(self, session_id, task):
        # GPU0ì— Planning ì‘ì—… íì‰
        await self.gpu0_queue.put((session_id, task))

    async def schedule_coding(self, session_id, task):
        # GPU1ì— Coding ì‘ì—… íì‰ (ë³‘ë ¬ ì²˜ë¦¬)
        await self.gpu1_queue.put((session_id, task))
```

**Option B: Review ë‹¨ê³„ë„ ë³‘ë ¬í™”**
```python
# Reviewë¥¼ DeepSeek-R1ì—ì„œ ìˆ˜í–‰í•˜ë„ë¡ ë¶„ë¦¬
class OptimizedWorkflow:
    def __init__(self):
        self.review_llm = ChatOpenAI(
            base_url=settings.vllm_reasoning_endpoint,  # GPU 0 ì‚¬ìš©
            model=settings.reasoning_model
        )
```

**ì˜ˆìƒ íš¨ê³¼:**
- GPU 0 í™œìš©ë„ ì¦ê°€ (20% â†’ 60%+)
- ë‹¤ì¤‘ ì‚¬ìš©ì í™˜ê²½ì—ì„œ ì²˜ë¦¬ëŸ‰ 3ë°° í–¥ìƒ
- GPU ìœ íœ´ ì‹œê°„ ìµœì†Œí™”

---

### 3. vLLM ì„œë²„ ìµœì í™” ì„¤ì •

#### ì¶”ì²œ vLLM ì„œë²„ ì‹¤í–‰ íŒŒë¼ë¯¸í„°

**GPU 0 (DeepSeek-R1) - ì¶”ë¡  ìµœì í™”:**
```bash
vllm serve deepseek-ai/DeepSeek-R1 \
  --port 8001 \
  --gpu-memory-utilization 0.9 \
  --max-model-len 32768 \
  --max-num-seqs 16 \
  --max-num-batched-tokens 65536 \
  --enable-chunked-prefill \
  --tensor-parallel-size 1
```

**GPU 1 (Qwen3-Coder) - ì²˜ë¦¬ëŸ‰ ìµœì í™”:**
```bash
vllm serve Qwen/Qwen3-8B-Coder \
  --port 8002 \
  --gpu-memory-utilization 0.95 \
  --max-model-len 32768 \
  --max-num-seqs 64 \
  --max-num-batched-tokens 131072 \
  --enable-chunked-prefill \
  --enable-prefix-caching \
  --tensor-parallel-size 1
```

**ì£¼ìš” íŒŒë¼ë¯¸í„° ì„¤ëª…:**
- `--max-num-seqs`: ë™ì‹œ ì²˜ë¦¬ ì‹œí€€ìŠ¤ ìˆ˜ (ë†’ì¼ìˆ˜ë¡ ì²˜ë¦¬ëŸ‰ ì¦ê°€)
- `--max-num-batched-tokens`: ë°°ì¹˜ë‹¹ ìµœëŒ€ í† í° ìˆ˜
- `--enable-chunked-prefill`: ê¸´ í”„ë¡¬í”„íŠ¸ íš¨ìœ¨ì  ì²˜ë¦¬
- `--enable-prefix-caching`: ë°˜ë³µë˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìºì‹±
- `--gpu-memory-utilization`: GPU ë©”ëª¨ë¦¬ í™œìš©ë¥  (0.9-0.95 ê¶Œì¥)

**ì˜ˆìƒ íš¨ê³¼:**
- GPU 1ì—ì„œ 25ê°œ ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ê°€ëŠ¥
- ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìºì‹±ìœ¼ë¡œ 10-15% ì†ë„ í–¥ìƒ
- Chunked prefillë¡œ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ íš¨ìœ¨ ì¦ê°€

---

### 4. ì ì‘í˜• ë°°ì¹˜ í¬ê¸° ì¡°ì •

#### í˜„ì¬ ë¡œì§
```python
# workflow_manager.py:1609-1617
if self.adaptive_parallelism:
    optimal_parallel = min(len(grouped_checklist), self.max_parallel_agents)
    if len(grouped_checklist) <= 5:
        optimal_parallel = len(grouped_checklist)
```

#### ê°œì„ ëœ ë¡œì§
```python
# H100 ì„±ëŠ¥ì„ ê³ ë ¤í•œ ì ì‘í˜• ì¡°ì •
def calculate_optimal_parallel(self, task_count: int) -> int:
    """Calculate optimal parallelism based on H100 capabilities."""
    # H100 ê¸°ì¤€ ê¶Œì¥ ë™ì‹œ ì²˜ë¦¬ ìˆ˜
    H100_RECOMMENDED_PARALLEL = 25

    if task_count <= 10:
        # ì†Œê·œëª¨ ì‘ì—…: ëª¨ë‘ ë³‘ë ¬ ì²˜ë¦¬
        return task_count
    elif task_count <= 20:
        # ì¤‘ê·œëª¨ ì‘ì—…: ì ê·¹ì  ë³‘ë ¬í™”
        return min(task_count, H100_RECOMMENDED_PARALLEL)
    else:
        # ëŒ€ê·œëª¨ ì‘ì—…: ìµœëŒ€ ë³‘ë ¬ë„ ìœ ì§€
        return H100_RECOMMENDED_PARALLEL
```

**ì˜ˆìƒ íš¨ê³¼:**
- ì‘ì€ í”„ë¡œì íŠ¸: ì¦‰ì‹œ ì™„ë£Œ (ëª¨ë“  ì‘ì—… ë™ì‹œ ì‹¤í–‰)
- ì¤‘ê°„ í”„ë¡œì íŠ¸: 25ê°œ ì‘ì—… ë™ì‹œ ì²˜ë¦¬ë¡œ ë¹ ë¥¸ ì™„ë£Œ
- í° í”„ë¡œì íŠ¸: ì•ˆì •ì ì¸ ë°°ì¹˜ ì²˜ë¦¬

---

### 5. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ë©”íŠ¸ë¦­

#### ì¶”ê°€ ê¶Œì¥ ë©”íŠ¸ë¦­
```python
class PerformanceMetrics:
    """Track GPU utilization and workflow performance."""

    def __init__(self):
        self.gpu0_utilization = []  # DeepSeek-R1 í™œìš©ë„
        self.gpu1_utilization = []  # Qwen3-Coder í™œìš©ë„
        self.parallel_task_count = []  # ë™ì‹œ ì‹¤í–‰ ì‘ì—… ìˆ˜
        self.task_latency = []  # ì‘ì—…ë³„ ì§€ì—° ì‹œê°„

    async def log_metrics(self):
        """Log current GPU utilization."""
        # nvidia-smië¥¼ í†µí•œ GPU ëª¨ë‹ˆí„°ë§
        # vLLM metrics endpoint í™œìš©
        pass
```

#### vLLM ë©”íŠ¸ë¦­ í™œìš©
```python
# vLLMì€ /metrics ì—”ë“œí¬ì¸íŠ¸ ì œê³µ
async def get_vllm_metrics():
    gpu0_metrics = await fetch("http://localhost:8001/metrics")
    gpu1_metrics = await fetch("http://localhost:8002/metrics")

    return {
        "gpu0_throughput": gpu0_metrics["throughput"],
        "gpu1_throughput": gpu1_metrics["throughput"],
        "gpu0_queue_size": gpu0_metrics["queue_size"],
        "gpu1_queue_size": gpu1_metrics["queue_size"]
    }
```

---

## ìš°ì„ ìˆœìœ„ë³„ ì ìš© ê³„íš

### Phase 1: ì¦‰ì‹œ ì ìš© (10ë¶„)
1. âœ… `max_parallel_agents` 10 â†’ 25ë¡œ ì¦ê°€
2. âœ… vLLM ì„œë²„ ì¬ì‹œì‘ (ìµœì í™”ëœ íŒŒë¼ë¯¸í„°)

**ì˜ˆìƒ íš¨ê³¼:** ì²˜ë¦¬ ì†ë„ 2ë°° í–¥ìƒ

### Phase 2: ë‹¨ê¸° (1ì¼)
3. ğŸ”„ ì ì‘í˜• ë°°ì¹˜ í¬ê¸° ë¡œì§ ê°œì„ 
4. ğŸ”„ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œìŠ¤í…œ ì¶”ê°€

**ì˜ˆìƒ íš¨ê³¼:** ì¶”ê°€ 20% ì„±ëŠ¥ í–¥ìƒ, ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥

### Phase 3: ì¤‘ê¸° (1ì£¼)
5. ğŸš€ ì„¸ì…˜ ê°„ íŒŒì´í”„ë¼ì´ë‹ êµ¬í˜„
6. ğŸš€ Review ë‹¨ê³„ GPU 0ìœ¼ë¡œ ë¶„ë¦¬

**ì˜ˆìƒ íš¨ê³¼:** ë‹¤ì¤‘ ì‚¬ìš©ì í™˜ê²½ì—ì„œ 3ë°° ì²˜ë¦¬ëŸ‰ ì¦ê°€

---

## ì„±ëŠ¥ ë¹„êµ (ì˜ˆìƒ)

### ë‹¨ì¼ ì„¸ì…˜ ì›Œí¬í”Œë¡œìš° (20ê°œ ì‘ì—…)

| êµ¬ì„± | Planning | Coding | Review | ì´ ì‹œê°„ |
|------|----------|--------|--------|---------|
| **í˜„ì¬** | 10ì´ˆ | 40ì´ˆ (10ê°œ ë³‘ë ¬) | 15ì´ˆ | **65ì´ˆ** |
| **Phase 1** | 10ì´ˆ | 16ì´ˆ (25ê°œ ë³‘ë ¬) | 15ì´ˆ | **41ì´ˆ** (37% ë‹¨ì¶•) |
| **Phase 3** | 10ì´ˆ | 16ì´ˆ (25ê°œ ë³‘ë ¬) | 15ì´ˆ | **41ì´ˆ** (37% ë‹¨ì¶•) |

### ë‹¤ì¤‘ ì„¸ì…˜ (3ê°œ ë™ì‹œ ì‹¤í–‰)

| êµ¬ì„± | GPU 0 í™œìš©ë„ | GPU 1 í™œìš©ë„ | ì´ ì²˜ë¦¬ ì‹œê°„ |
|------|--------------|--------------|--------------|
| **í˜„ì¬** | 20% | 60% | **195ì´ˆ** (65ì´ˆ Ã— 3) |
| **Phase 1** | 20% | 80% | **123ì´ˆ** (41ì´ˆ Ã— 3) |
| **Phase 3** | 65% | 85% | **70ì´ˆ** (íŒŒì´í”„ë¼ì´ë‹) |

---

## êµ¬í˜„ ì½”ë“œ ì˜ˆì‹œ

### 1. max_parallel_agents ì¦ê°€
```python
# backend/app/agent/langchain/workflow_manager.py:531
self.max_parallel_agents = 25  # H100 ìµœì í™”
```

### 2. ê°œì„ ëœ ì ì‘í˜• ë¡œì§
```python
# workflow_manager.pyì— ì¶”ê°€
def calculate_optimal_parallel(self, task_count: int) -> int:
    """Calculate optimal parallelism for H100 GPUs."""
    H100_MAX_PARALLEL = 25

    if task_count <= 10:
        return task_count
    elif task_count <= 25:
        return min(task_count, H100_MAX_PARALLEL)
    else:
        return H100_MAX_PARALLEL

# _execute_parallel_coding ë©”ì„œë“œ ìˆ˜ì •
optimal_parallel = self.calculate_optimal_parallel(len(grouped_checklist))
```

### 3. GPU ë¶„ì‚° Review
```python
# workflow_manager.py __init__ ìˆ˜ì •
self.review_llm = ChatOpenAI(
    base_url=settings.vllm_reasoning_endpoint,  # GPU 0ìœ¼ë¡œ ë³€ê²½
    model=settings.reasoning_model,
    temperature=0.3,
    streaming=True
)
```

---

## ëª¨ë‹ˆí„°ë§ ë° ê²€ì¦

### ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
```python
# performance_test.py
import asyncio
import time

async def test_parallel_performance():
    """Test parallel coding performance."""
    tasks = [
        {"task": f"Create file_{i}.py", "complexity": "simple"}
        for i in range(30)
    ]

    start = time.time()
    # Execute workflow with tasks
    elapsed = time.time() - start

    print(f"Processed {len(tasks)} tasks in {elapsed:.2f}s")
    print(f"Throughput: {len(tasks)/elapsed:.2f} tasks/sec")
```

### GPU í™œìš©ë„ ëª¨ë‹ˆí„°ë§
```bash
# ì‹¤ì‹œê°„ GPU ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# vLLM ë©”íŠ¸ë¦­ í™•ì¸
curl http://localhost:8001/metrics | grep throughput
curl http://localhost:8002/metrics | grep throughput
```

---

## ê²°ë¡ 

**ì¦‰ì‹œ ì ìš© ê¶Œì¥ (Phase 1):**
1. `max_parallel_agents = 25`ë¡œ ì¦ê°€
2. vLLM ì„œë²„ ìµœì í™” íŒŒë¼ë¯¸í„°ë¡œ ì¬ì‹œì‘

**ì˜ˆìƒ ì´ ê°œì„ :**
- ë‹¨ì¼ ì„¸ì…˜: 37% ì†ë„ í–¥ìƒ
- ë‹¤ì¤‘ ì„¸ì…˜: 64% ì†ë„ í–¥ìƒ (Phase 3ê¹Œì§€)
- GPU í™œìš©ë„: 40% â†’ 75%+ ì¦ê°€

H100 2ê°œì˜ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ì¶©ë¶„íˆ í™œìš©í•˜ì§€ ëª»í•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤. ìœ„ ìµœì í™”ë¡œ í•˜ë“œì›¨ì–´ ì ì¬ë ¥ì„ ìµœëŒ€í•œ ëŒì–´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
